{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM91vIAqKOexBOcxq9R4ikn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anshad-Aziz/PRODIGY_ML_05/blob/main/Task_05_FOOD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqgEd9GRH4AZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_list=tfds.list_builders()\n",
        "\n",
        "print(\"food101\" in dataset_list)\n",
        "(train_data,test_data),ds_info=tfds.load(name='food101',\n",
        "                                         split=[\"train\",\"validation\"],\n",
        "                                         shuffle_files=True,\n",
        "                                         as_supervised=True,  # give in tuple (data,label)\n",
        "                                         with_info=True)\n",
        "# meta data\n",
        "ds_info.features\n",
        "# class names of data\n",
        "class_names=ds_info.features[\"label\"].names\n",
        "class_names[:10]\n",
        "# minimum and maximum pixel value\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reduce_min(x[0]),tf.reduce_max(x[0])"
      ],
      "metadata": {
        "id": "exa10HOvH5fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing function\n",
        "\n",
        "def preprocess_img(image,label,image_shape=224):\n",
        "  \"\"\" convert image dtype from unit8 to float32,reshapes\n",
        "      image to [image_shape,image_shape,color_channel]\n",
        "  \"\"\"\n",
        "  image=tf.image.resize(image,[image_shape,image_shape]) # target image\n",
        "  return tf.cast(image,tf.float32),label\n",
        "sample_image=x[0]\n",
        "preprocessed_image=preprocess_img(x[0],x[1])[0]\n",
        "\n",
        "print(\"Before preprocessing...\")\n",
        "print(f\"\"\"\n",
        "sample image shape : {sample_image.shape}\n",
        "sample image dtype : {sample_image.dtype}\n",
        "\"\"\")\n",
        "\n",
        "print(\"After preprocessing...\")\n",
        "print(f\"\"\"\n",
        "preprocessed image shape : {preprocessed_image.shape}\n",
        "preprocessed image dtype : {preprocessed_image.dtype}\n",
        "\"\"\")\n",
        "# map preprocess function to training data (by parallel processing)\n",
        "train_data=train_data.map(map_func=preprocess_img,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# shuffle our training data then make batches and prefetch it for faster to load\n",
        "train_data=train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# now same process for testing data\n",
        "test_data=test_data.map(map_func=preprocess_img,num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size=32).prefetch(buffer_size=1000)\n",
        "train_data,test_data\n"
      ],
      "metadata": {
        "id": "gvhps_HiH5jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "len(class_names)\n",
        "\n",
        "Data_augumentaion=tf.keras.Sequential([\n",
        "  preprocessing.RandomFlip('horizontal'),\n",
        "  preprocessing.RandomRotation(0.2),\n",
        "  preprocessing.RandomZoom(0.2),\n",
        "  preprocessing.RandomHeight(0.2),\n",
        "  preprocessing.RandomWidth(0.2)\n",
        "],name=\"data_augumentation\")\n",
        "\n",
        "base_model=tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable=False\n",
        "\n",
        "input=layers.Input(shape=(224,224,3),name='input_layer')\n",
        "x=Data_augumentaion(input)\n",
        "x=base_model(x,training=False)\n",
        "\n",
        "x=layers.GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\n",
        "x=layers.Dense(len(class_names))(x)\n",
        "output=layers.Activation(activation='softmax',dtype=tf.float32,name='softmax_mixed_output_layer')(x)\n",
        "\n",
        "model=tf.keras.Model(input,output)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "for layer in model.layers:\n",
        "  print(layer.name,layer.trainable,layer.dtype,layer.dtype_policy)\n",
        "\n",
        "history_101=model.fit(train_data,\n",
        "                      epochs=3,  # prefer 3 epochs ,over it overfitting occur\n",
        "                      steps_per_epoch=len(train_data),\n",
        "                      validation_data=test_data,\n",
        "                      validation_steps=int(0.15*len(test_data)),\n",
        "                      )\n",
        "model.evaluate(test_data)"
      ],
      "metadata": {
        "id": "1q4Bn8G8H5ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unfreeze some layers\n",
        "base_model.trainable=True\n",
        "\n",
        "for x in base_model.layers[:-25]:\n",
        "  x.trainable=False\n",
        "\n",
        "\n",
        "initial_epoch=history_101.epoch\n",
        "\n",
        "fine_tune_epoch=len(initial_epoch)+2\n",
        "\n",
        "history_101_fine_tune=model.fit(train_data,\n",
        "                    epochs=fine_tune_epoch,\n",
        "                    validation_data=test_data,\n",
        "                    validation_steps=int(0.15*len(test_data)),\n",
        "                    initial_epoch=len(initial_epoch)-1)\n",
        "\n",
        "initial_epoch=history_101_fine_tune.epoch\n",
        "\n",
        "fine_tune_epoch=len(initial_epoch)+5\n",
        "\n",
        "history=model.fit(train_data,\n",
        "                    epochs=fine_tune_epoch,\n",
        "                    validation_data=test_data,\n",
        "                    validation_steps=int(0.15*len(test_data)),\n",
        "                    initial_epoch=len(initial_epoch)-1)\n",
        "model.evaluate(test_data)\n",
        "tf.keras.models.save_model(model,\"fine_tune_model_1.h5\")\n",
        "\n",
        "tf.keras.models.save_model(model,\"/content/drive/MyDrive/Colab Notebooks/models/food_vision_fine_tune_model_1.h5\")\n",
        "Save Model\n",
        "loaded_model=tf.keras.models.load_model('/content/07_efficientnetb0_fine_tuned_101_classes_mixed_precision')\n",
        "loaded_model.evaluate(test_data)# PRODIGY_ML_05"
      ],
      "metadata": {
        "id": "CtUktjmtH55o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}